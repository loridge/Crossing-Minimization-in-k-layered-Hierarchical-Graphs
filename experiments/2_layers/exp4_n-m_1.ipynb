{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4 Bipartite n-m graphs experiment (m-layer free)\n",
    "\n",
    "A two layer experiment that aims to benchmark barycenter, sifting, and median heuristic algorithms to n-m bipartite graphs generated using NetworkX.\n",
    "In this experiment, singleton nodes are now considered to provide the correct density for smaller graphs since smaller graphs are computationally feasible in our study. The m-layer is the free/reorderable layer in this experiment.\n",
    "\n",
    "(may error ito, use the py version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For debugging, most of the functions presented here were copied from the original source files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design \n",
    "\n",
    "n-m bipartite graphs with singleton consideration\n",
    "- n is even (4 6 8 10)\n",
    "- for every n, m is  range[n/2,n],step=1\n",
    "\n",
    "For bipartite graphs, where the singletons are placed does not matter when it comes to solutions. \n",
    "The free-layer is the m set of the bipartite graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext cython\n"
     ]
    }
   ],
   "source": [
    "# Imports \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import networkx as nx \n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from networkx.algorithms import bipartite\n",
    "from itertools import combinations, permutations\n",
    "from typing import Dict, Union, List, Set\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph generator/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forced_density_gen_bip_graph(n1, n2, density):\n",
    "    \"\"\"\n",
    "    Generate a bipartite graph with a specified edge density.\n",
    "\n",
    "    Args:\n",
    "        n1 (int): Number of nodes in the first partition (layer 0).\n",
    "        n2 (int): Number of nodes in the second partition (layer 1).\n",
    "        density (float): Desired edge density (0 < density â‰¤ 1), defined as |E| / (|V1| * |V2|).\n",
    "\n",
    "    Returns:\n",
    "        tuple: (nodes, edges, B, top_nodes, bottom_nodes)\n",
    "            - nodes: List of dictionaries with \"id\" and \"depth\".\n",
    "            - edges: List of dictionaries with \"nodes\" as a pair of connected node IDs.\n",
    "            - B: NetworkX bipartite graph.\n",
    "            - top_nodes: Set of nodes in the first partition.\n",
    "            - bottom_nodes: Set of nodes in the second partition.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize bipartite graph\n",
    "    B = nx.Graph()\n",
    "    top_nodes = set(range(1, n1 + 1))\n",
    "    bottom_nodes = set(range(n1 + 1, n1 + n2 + 1))\n",
    "\n",
    "    B.add_nodes_from(top_nodes, bipartite=0)\n",
    "    B.add_nodes_from(bottom_nodes, bipartite=1)\n",
    "\n",
    "    # Compute the exact number of edges required\n",
    "    max_edges = n1 * n2\n",
    "    num_edges = max(1, min(int(math.ceil(density * max_edges)), max_edges))  # Ensure valid range\n",
    "\n",
    "    edges = set()\n",
    "\n",
    "    # Step 1: Shuffle\n",
    "    top_list = list(top_nodes)\n",
    "    bottom_list = list(bottom_nodes)\n",
    "    random.shuffle(top_list)\n",
    "    random.shuffle(bottom_list)\n",
    "\n",
    "    # note that katapat nya yung meron, \n",
    "    # for i in range(max(n1, n2)):\n",
    "    #     u = top_list[i % n1]  # Cycle through top nodes\n",
    "    #     v = bottom_list[i % n2]  # Cycle through bottom nodes\n",
    "    #     edges.add((u, v))\n",
    "    #     B.add_edge(u, v)\n",
    "\n",
    "    # Step 2: Randomly add edges based on density (no forced connections)\n",
    "    while len(edges) < num_edges:\n",
    "        u = random.choice(top_list)\n",
    "        v = random.choice(bottom_list)\n",
    "        if (u, v) not in edges:\n",
    "            edges.add((u, v))\n",
    "            B.add_edge(u, v)\n",
    "\n",
    "    # Convert to required format\n",
    "    nodes = [{\"id\": f\"u{node}\", \"depth\": 0} for node in top_nodes] + \\\n",
    "            [{\"id\": f\"u{node}\", \"depth\": 1} for node in bottom_nodes]\n",
    "\n",
    "    edges = [{\"nodes\": [f\"u{u}\", f\"u{v}\"]} for u, v in edges]\n",
    "    \n",
    "    return nodes, edges, B, top_nodes, bottom_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utilities\n",
    "\n",
    "- parse_edges\n",
    "- cross_count_optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_edges(edges, top_nodes, bottom_nodes):\n",
    "    \"\"\"\n",
    "    Parse edges from the given format and map them to integers corresponding to top and bottom nodes.\n",
    "    Args:\n",
    "        edges (list): List of edges in the format [{'nodes': ['u0', 'u6']}, ...].\n",
    "        top_nodes (list): List of top-layer node IDs (e.g., [0, 1, 2]).\n",
    "        bottom_nodes (list): List of bottom-layer node IDs (e.g., [3, 4, 5, 6, 7]).\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tuples representing edges as (top_node, bottom_node).\n",
    "    \"\"\"\n",
    "    parsed_edges = []\n",
    "    for edge in edges:\n",
    "        u, v = edge['nodes']\n",
    "        # Convert 'uX' to integer node IDs\n",
    "        u_id = int(u[1:])  # Remove 'u' and convert to integer\n",
    "        v_id = int(v[1:])\n",
    "        if u_id in top_nodes and v_id in bottom_nodes:\n",
    "            parsed_edges.append((u_id, v_id))\n",
    "        elif v_id in top_nodes and u_id in bottom_nodes:\n",
    "            parsed_edges.append((v_id, u_id))\n",
    "    # print(\"DEBUG: parsed_edges internal\", parsed_edges, \"vs\", edges, \"nodes\",top_nodes, bottom_nodes)\n",
    "    return parsed_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search_first_smaller(arr, v, lower_bound, upper_bound, index_references, v_index):\n",
    "    \"\"\"\n",
    "    Binary search to find the rightmost index in 'arr' where the value is smaller than 'v'.\n",
    "    The search starts from 'lower_bound' and ends at 'upper_bound' to optimize performance.\n",
    "\n",
    "    Args:\n",
    "        arr (list[str]): The sorted list of neighbor nodes.\n",
    "        v (str): The node to compare against.\n",
    "        lower_bound (int): The starting index for the search.\n",
    "        upper_bound (int): The ending index for the search.\n",
    "        index_references (dict): Dictionary mapping nodes to their fixed_layer indices.\n",
    "        v_index (int): The index of the node 'v' in the fixed layer.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the last element smaller than 'v', or -1 if none exist.\n",
    "    \"\"\"\n",
    "    left, right = lower_bound, upper_bound\n",
    "    result = -1  # Default to -1 (not found)\n",
    "\n",
    "    while left <= right:  # Fix condition to include rightmost element\n",
    "        mid = (left + right) // 2\n",
    "        # print(f\"DEBUG INSIDE BINSEARCH arr[mid]: {arr[mid]}, left: {left}, right: {right}, mid: {mid}\")\n",
    "\n",
    "        if index_references[arr[mid]] < v_index:\n",
    "            result = mid  # Update result, but keep searching to the right\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1  # Move left to find a smaller value\n",
    "\n",
    "    return result  # Final rightmost valid index\n",
    "\n",
    "def cross_count_optimized(fixed_layer: list[str], free_layer: list[str], edges: list):\n",
    "    crossing_total = 0\n",
    "    \n",
    "    fixed_layer = [f\"u{node}\" if isinstance(node, int) else node for node in list(fixed_layer) ]\n",
    "    free_layer =  [f\"u{node}\" if isinstance(node, int) else node for node in list(free_layer) ]\n",
    "\n",
    "    fixed_layer_dict = {node: index for index, node in enumerate(fixed_layer)}\n",
    "    free_layer_dict = {node: index for index, node in enumerate(free_layer)}\n",
    "\n",
    "    neighbor_dict = {node: [] for node in free_layer}\n",
    "    easy_free = set(free_layer)\n",
    "    easy_fixed = set(fixed_layer)\n",
    "\n",
    "    for edge_data in edges:\n",
    "        u, v = edge_data[\"nodes\"]\n",
    "        if u in easy_free and v in easy_fixed:\n",
    "            neighbor_dict[u].append(v)\n",
    "        elif v in easy_free and u in easy_fixed:\n",
    "            neighbor_dict[v].append(u)\n",
    "\n",
    "    # Sort neighbors based on their position in fixed_layer\n",
    "    for node in neighbor_dict:\n",
    "        neighbor_dict[node].sort(key=lambda x: fixed_layer_dict[x])\n",
    "\n",
    "    #### CROSSING PROPER ####\n",
    "    for i, u_node in enumerate(free_layer):\n",
    "        u_neighbors = neighbor_dict[u_node]\n",
    "        u_prime_nodes = free_layer[i + 1:]\n",
    "        # print(\"\")\n",
    "        # print(\"u_node \", u_node, \";;;u_prime nodes > u_node: \",u_prime_nodes)\n",
    "        for u_prime in u_prime_nodes:\n",
    "          u_prime_neighbors = neighbor_dict[u_prime]\n",
    "          lb = 0   # 0 indexed as opposed to pseudocode\n",
    "          ub = len(u_prime_neighbors) - 1  # 0 indexed as opposed to pseudocode\n",
    "          # print(f\"DEBUG u-prime-neighbors: {u_prime_neighbors} of u-prime {u_prime}\")\n",
    "          for v in u_neighbors:\n",
    "              result = binary_search_first_smaller(u_prime_neighbors, v, lb, ub, fixed_layer_dict, fixed_layer_dict[v]) ##, edit it must be based on indices not the values of the elements themselves\n",
    "              if result != -1:\n",
    "                crossing_total += result + 1\n",
    "\n",
    "    return crossing_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_crossings(fixed_layer, free_layer, edges):\n",
    "    \"\"\"\n",
    "    Find the optimal ordering of the free layer to minimize edge crossings.\n",
    "\n",
    "    Parameters:\n",
    "    - fixed_layer: List of vertices in the fixed layer (fixed order).\n",
    "    - free_layer: List of vertices in the free layer.\n",
    "    - edges: List of dictionaries with 'nodes' key, each containing a list of two vertices representing an edge.\n",
    "\n",
    "    Returns:\n",
    "    - Optimal ordering of free_layer with minimal crossings.\n",
    "    - Minimum number of crossings.\n",
    "    \"\"\"\n",
    "    min_crossings = float('inf')\n",
    "    optimal_ordering = None\n",
    "    # print(\"Currently has\", len(fixed_layer), \"vertices\",  edges)\n",
    "    for perm in permutations(free_layer):\n",
    "        current_crossings = cross_count_optimized(fixed_layer, list(perm), edges)\n",
    "        if current_crossings < min_crossings:\n",
    "            min_crossings = current_crossings\n",
    "            optimal_ordering = perm\n",
    "\n",
    "    return list(optimal_ordering), min_crossings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barycenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barycenter(bottom_nodes, top_nodes, edges):\n",
    "    # Ensure top_nodes is a list\n",
    "    top_nodes = list(top_nodes)\n",
    "\n",
    "    # Create a dictionary to store the neighbors of each bottom node\n",
    "    neighbors = {node: set() for node in bottom_nodes}\n",
    "    for u, v in edges:\n",
    "        if u in top_nodes:\n",
    "            neighbors[v].add(u)\n",
    "        else:\n",
    "            neighbors[u].add(v)\n",
    "\n",
    "    # Calculate barycenter values for bottom nodes\n",
    "    barycenter_values = {}\n",
    "    for node in bottom_nodes:\n",
    "        if len(neighbors[node]) > 0:\n",
    "            barycenter_values[node] = sum(top_nodes.index(n) + 1 for n in neighbors[node]) / len(neighbors[node])\n",
    "        else:\n",
    "            barycenter_values[node] = float('inf')  # Assign a very high value for isolated nodes\n",
    "\n",
    "    # Sort bottom nodes based on barycenter values\n",
    "    sorted_bottom_nodes = sorted(bottom_nodes, key=lambda x: barycenter_values[x])\n",
    "\n",
    "    return sorted_bottom_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(bottom_nodes, top_nodes, edges):\n",
    "    \"\"\"\n",
    "    Reorders bottom nodes using the median heuristic.\n",
    "    \n",
    "    Parameters:\n",
    "    - bottom_nodes: List of bottom-layer nodes.\n",
    "    - top_nodes: List of top-layer nodes.\n",
    "    - edges: List of tuples representing edges (top_node, bottom_node).\n",
    "    \n",
    "    Returns:\n",
    "    - Reordered list of bottom-layer nodes.\n",
    "    \"\"\"\n",
    "    # Ensure top_nodes is a list\n",
    "    top_nodes = list(top_nodes)\n",
    "    # Dictionary to store neighbors of each bottom node\n",
    "    neighbors = {node: [] for node in bottom_nodes}\n",
    "    \n",
    "    # Populate neighbors dictionary\n",
    "    for u, v in edges:\n",
    "        if u in top_nodes and v in bottom_nodes:\n",
    "            neighbors[v].append(top_nodes.index(u) + 1)\n",
    "        elif v in top_nodes and u in bottom_nodes:\n",
    "            neighbors[u].append(top_nodes.index(v) + 1)\n",
    "    \n",
    "    # Compute the median for each bottom node\n",
    "    median_values = {}\n",
    "    for node, positions in neighbors.items():\n",
    "        if positions:\n",
    "            sorted_positions = sorted(positions)\n",
    "            n = len(sorted_positions)\n",
    "            # Compute median\n",
    "            if n % 2 == 1:\n",
    "                median_values[node] = sorted_positions[n // 2]\n",
    "            else:\n",
    "                median_values[node] = (sorted_positions[n // 2 - 1] + sorted_positions[n // 2]) / 2\n",
    "        else:\n",
    "            median_values[node] = float('inf')  # Nodes with no neighbors go to the end\n",
    "    \n",
    "    # Sort bottom nodes by their median values\n",
    "    sorted_bottom_nodes = sorted(bottom_nodes, key=lambda x: median_values[x])\n",
    "    \n",
    "    return sorted_bottom_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sifting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sifting(bottom_nodes, top_nodes, edges, verbose=0):\n",
    "    \"\"\"\n",
    "    Reorders bottom nodes using the sifting heuristic based on indegree in decreasing order.\n",
    "    \n",
    "    Parameters:\n",
    "    - bottom_nodes: List of bottom-layer nodes.\n",
    "    - top_nodes: List of top-layer nodes.\n",
    "    - edges: List of dictionaries representing edges with format {'nodes': ['uX', 'uY']}.\n",
    "    \n",
    "    Returns:\n",
    "    - Reordered list of bottom-layer nodes as integers.\n",
    "    \"\"\"\n",
    "    \n",
    "    top_nodes = [f\"u{node}\" if isinstance(node, int) else node for node in list(top_nodes) ]\n",
    "    bottom_nodes = [f\"u{node}\" if isinstance(node, int) else node for node in list(bottom_nodes) ]  \n",
    "    \n",
    "    # Compute indegree for each bottom node\n",
    "    indegree = {node: 0 for node in bottom_nodes}\n",
    "    for edge in edges:\n",
    "        _, b = edge['nodes']\n",
    "        indegree[b] += 1\n",
    "    \n",
    "    # Sort bottom nodes by indegree in decreasing order (priority queue for processing order)\n",
    "    sorted_nodes = sorted(bottom_nodes, key=lambda node: -indegree[node])\n",
    "    \n",
    "    # Apply the sifting heuristic\n",
    "    for node in sorted_nodes:\n",
    "        best_position = bottom_nodes.index(node)\n",
    "        best_crossings = cross_count_optimized(top_nodes, bottom_nodes, edges)\n",
    "        \n",
    "        for j in range(len(bottom_nodes)):\n",
    "            if bottom_nodes[j] == node:\n",
    "                continue\n",
    "            \n",
    "            # Swap node to new position\n",
    "            bottom_nodes.remove(node)\n",
    "            bottom_nodes.insert(j, node)\n",
    "            current_crossings = cross_count_optimized(top_nodes, bottom_nodes, edges)\n",
    "            \n",
    "            if current_crossings < best_crossings:\n",
    "                best_position = j\n",
    "                best_crossings = current_crossings\n",
    "            \n",
    "            # Revert swap\n",
    "            bottom_nodes.remove(node)\n",
    "            bottom_nodes.insert(best_position, node)\n",
    "    \n",
    "    # Extract integer values from node labels\n",
    "    return [int(node[1:]) if isinstance(node, str) and node.startswith('u') and node[1:].isdigit() else node for node in bottom_nodes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment (may error ito, use the py version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp_4 4-2 underway\n",
      "Running experiment for n:4, m:2, p:0.1\n",
      "Running experiment for n:4, m:2, p:0.2\n",
      "Running experiment for n:4, m:2, p:0.3\n",
      "Running experiment for n:4, m:2, p:0.4\n",
      "Running experiment for n:4, m:2, p:0.5\n",
      "Running experiment for n:4, m:2, p:0.6\n",
      "Running experiment for n:4, m:2, p:0.7\n",
      "Running experiment for n:4, m:2, p:0.8\n",
      "Running experiment for n:4, m:2, p:0.9\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 106\u001b[0m\n\u001b[0;32m    103\u001b[0m         futures\u001b[38;5;241m.\u001b[39mappend(executor\u001b[38;5;241m.\u001b[39msubmit(run_experiment, n, m, p, num_samples))\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[1;32m--> 106\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    108\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    109\u001b[0m total_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "def run_experiment(n1, n2, p, num_samples):\n",
    "    total_density=0\n",
    "    # total_density=[]\n",
    "    total_actual_density=0\n",
    "    max_edges=n1*n2\n",
    "    result = {\n",
    "        \"n1\": n1,\n",
    "        \"n2\": n2,\n",
    "        \"num_samples\": num_samples,\n",
    "        \"avg_actual_edges\": 0,\n",
    "        \"max_edges\": max_edges,\n",
    "        \"pred_density\": None,\n",
    "        \"density\": None,\n",
    "        \"avg_crossings_original\": 0,\n",
    "        \"avg_crossings_barycenter\": 0,\n",
    "        \"avg_crossings_median\": 0,\n",
    "        \"avg_crossings_sifting\": 0,\n",
    "        \"avg_crossings_optimal\": 0\n",
    "    }\n",
    "    for _ in range(num_samples):\n",
    "        # Generate bipartite graph\n",
    "        nodes, edges, B, top_nodes, bottom_nodes = forced_density_gen_bip_graph(n1, n2, p)\n",
    "        \n",
    "        # Calculate density, not yet bipartite\n",
    "        density = bipartite.density(B, set(top_nodes))\n",
    "        total_density += density\n",
    "        # total_density.append(density)\n",
    "        \n",
    "        num_edges = max(1, min(int(math.ceil(density * max_edges)), max_edges))  # Ensure valid range\n",
    "        total_actual_density += num_edges\n",
    "        \n",
    "        crossings_original = crossings_median = cross_count_optimized(top_nodes, bottom_nodes, edges)\n",
    "\n",
    "        # Parse the edges into (top_node, bottom_node) tuples before passing to the barycenter function\n",
    "        parsed_edges = parse_edges(edges, top_nodes, bottom_nodes)\n",
    "        \n",
    "        # Apply Barycenter heuristic to reorder bottom nodes\n",
    "        bottom_nodes_bary = barycenter(bottom_nodes, top_nodes, parsed_edges)\n",
    "\n",
    "        # Update positions: top nodes fixed, bottom nodes reordered\n",
    "        # pos_barycenter = update_positions(top_nodes, bottom_nodes_bary)\n",
    "        # crossings_barycenter = count_crossings(B, pos_barycenter)\n",
    "        crossings_barycenter = cross_count_optimized(top_nodes, bottom_nodes_bary, edges)\n",
    "\n",
    "        # Apply Median heuristic to reorder bottom nodes\n",
    "        bottom_nodes_median = median(bottom_nodes, top_nodes, parsed_edges)\n",
    "        crossings_median = cross_count_optimized(top_nodes, bottom_nodes_median, edges)\n",
    "\n",
    "        # Update positions: top nodes fixed, bottom nodes reordered\n",
    "        # pos_median = update_positions(top_nodes, bottom_nodes_median)\n",
    "        # crossings_median = count_crossings(B, pos_median)\n",
    "\n",
    "        # Apply Simple Sifting heuristic to reorder bottom nodes\n",
    "        sifting_heuristic = sifting(bottom_nodes, top_nodes, edges, verbose=0, )\n",
    "        crossings_sifting = cross_count_optimized(top_nodes, sifting_heuristic,edges)\n",
    "        \n",
    "        bottom_nodes_optimal, crossings_optimal = minimize_crossings(list(top_nodes), list(bottom_nodes), edges)\n",
    "        \n",
    "        result[\"avg_crossings_original\"] += crossings_original\n",
    "        result[\"avg_crossings_barycenter\"] += crossings_barycenter\n",
    "        result[\"avg_crossings_median\"] += crossings_median\n",
    "        result[\"avg_crossings_sifting\"] += crossings_sifting\n",
    "        result[\"avg_crossings_optimal\"] += crossings_optimal\n",
    "\n",
    "\n",
    "    # Store results\n",
    "    result[\"avg_actual_edges\"] = total_actual_density / num_samples\n",
    "    result[\"density\"] = total_density / num_samples\n",
    "    # result[\"density\"]=max(total_density)\n",
    "    result[\"pred_density\"] = p\n",
    "    result[\"avg_crossings_original\"] /= num_samples\n",
    "    result[\"avg_crossings_barycenter\"] /= num_samples\n",
    "    result[\"avg_crossings_median\"] /= num_samples\n",
    "    result[\"avg_crossings_sifting\"] /= num_samples\n",
    "    result[\"avg_crossings_optimal\"] /= num_samples\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #- n is even (4 6 8 10)\n",
    "    # - for every n, m is  range[n/2,n],step=1\n",
    "\n",
    "    # all possible n values\n",
    "    n_values=[4,6,8,10]\n",
    "\n",
    "    # singleton experiment with n-m graphs\n",
    "    for n in n_values:\n",
    "        ########## EVERY EXPERIMENT IS UNDER N-M ############\n",
    "        for m in range(n//2, n+1):\n",
    "            print(f\"Exp_4 {n}-{m} underway\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            \n",
    "            results=[]\n",
    "            num_samples = 10\n",
    "            p_values = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "            with ProcessPoolExecutor() as executor:\n",
    "                futures = []\n",
    "                # for n1 in n1_values:\n",
    "                #     for n2 in n2_values:\n",
    "                for p in p_values:\n",
    "                    print(f\"Running experiment for n:{n}, m:{m}, p:{p}\")\n",
    "                    futures.append(executor.submit(run_experiment, n, m, p, num_samples))\n",
    "\n",
    "                for future in futures:\n",
    "                    results.append(future.result())\n",
    "            \n",
    "            end_time = time.time()\n",
    "            total_time = end_time - start_time\n",
    "            print(f\"Total execution time: {total_time:.2f} seconds\")\n",
    "            \n",
    "            df = pd.DataFrame(results)\n",
    "            \n",
    "            plot_results_percentage_outliers(df, f\"{num_samples} Samples-generator is ceil\", \"exp4_n-m_1\", 'exp4')\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: <> (Most of the code are copy-pasted from their original files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
